{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQxBaVBB67HZXm0yyj6V90",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeiChenc/CDX-Tranche-Pricing/blob/main/CDX_Multi_Tenor_Gaussian_Copula_pricing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import norm\n",
        "from scipy.optimize import brentq\n",
        "from numpy.polynomial.hermite import hermgauss\n",
        "from scipy.interpolate import interp1d\n",
        "import datetime\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "eeWVzX1I7VNq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data\n",
        "    # Load files\n",
        "df_cdx = pd.read_csv('cdx_timeseries.csv', parse_dates=['Date'])\n",
        "df_ois = pd.read_csv('ois_timeseries.csv', parse_dates=['Date'])\n",
        "df_const = pd.read_csv('constituents_timeseries.csv', parse_dates=['Date'])\n",
        "\n",
        "    # Filter for 5Y Tenor mostly (Standard for CDX)\n",
        "df_cdx = df_cdx[df_cdx['Tenor'] == '5Y'].copy()\n",
        "\n",
        "    # Process OIS: Pivot to have dates as index, tenors as columns\n",
        "def parse_ois_tenor(t):\n",
        "        if 'Y' in t: return float(t.replace('Y', ''))\n",
        "        if 'M' in t: return float(t.replace('M', '')) / 12.0\n",
        "        if 'W' in t: return float(t.replace('W', '')) / 52.0\n",
        "        return 0.0\n",
        "\n",
        "df_ois['YearFrac'] = df_ois['Tenor'].apply(parse_ois_tenor)\n",
        "ois_pivot = df_ois.pivot(index='Date', columns='YearFrac', values='OIS_Rate')\n",
        "\n",
        "    # Process Constituents: Calculate Average Spread per day for LHP\n",
        "    # We need an \"Average 5Y Spread\" to represent the portfolio\n",
        "avg_spreads = df_const.groupby('Date')['Spread_5Y'].mean() / 10000.0 # Convert bps to decimal\n",
        "\n",
        "df_const, df_ois, df_cdx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFg3i_cLYTY9",
        "outputId": "056519e2-e4d0-4de3-ecd8-0a03191c7be1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(           Date           Company  Recovery  Spread_5Y  Spread_7Y  Spread_10Y\n",
              " 0    2024-11-19  Advanced Micro D     500.0      44.53      68.79       82.19\n",
              " 1    2024-11-19  Honeywell Intern     100.0      26.16      39.47       52.03\n",
              " 2    2024-11-19  Barrick Mining C     100.0      39.25      58.04       77.05\n",
              " 3    2024-11-19  American Electri     100.0      33.31      49.46       65.15\n",
              " 4    2024-11-19   American Expres     100.0      32.29      46.57       58.31\n",
              " ...         ...               ...       ...        ...        ...         ...\n",
              " 1095 2024-12-03  Valero Energy Co     100.0      51.00      80.46      101.88\n",
              " 1096 2024-12-03    Verizon Commun     100.0      61.07      78.11       97.43\n",
              " 1097 2024-12-03       Walmart Inc     100.0      26.07      39.71       51.42\n",
              " 1098 2024-12-03   Weyerhaeuser Co     100.0      32.15      45.59       61.65\n",
              " 1099 2024-12-03    Simon Property     100.0      52.90      85.24      102.43\n",
              " \n",
              " [1100 rows x 6 columns],\n",
              "           Date Tenor  OIS_Rate   YearFrac\n",
              " 0   2024-11-19    1W  3.892700   0.019231\n",
              " 1   2024-11-19    2W  3.894000   0.038462\n",
              " 2   2024-11-19    3W  3.886000   0.057692\n",
              " 3   2024-11-19    1M  3.865900   0.083333\n",
              " 4   2024-11-19    2M  3.842600   0.166667\n",
              " ..         ...   ...       ...        ...\n",
              " 283 2024-12-03   20Y  3.975261  20.000000\n",
              " 284 2024-12-03   25Y  3.993275  25.000000\n",
              " 285 2024-12-03   30Y  3.968186  30.000000\n",
              " 286 2024-12-03   40Y  3.865733  40.000000\n",
              " 287 2024-12-03   50Y  3.751356  50.000000\n",
              " \n",
              " [288 rows x 4 columns],\n",
              "          Date Tenor  Index_Bid  Index_Ask  Index_Last  Index_Mid  \\\n",
              " 3  2024-11-19    5Y     54.550     54.870     54.8686    54.7100   \n",
              " 9  2024-11-20    5Y     54.630     55.038     54.8250    54.8340   \n",
              " 15 2024-11-21    5Y     54.446     54.885     54.6650    54.6655   \n",
              " 21 2024-11-25    5Y     52.390     52.630     52.5640    52.5100   \n",
              " 27 2024-11-26    5Y     51.398     52.098     51.7480    51.7480   \n",
              " 33 2024-12-01    5Y     51.095     51.438     51.2670    51.2665   \n",
              " 39 2024-12-02    5Y     51.088     51.372     51.2300    51.2300   \n",
              " 45 2024-12-03    5Y     50.996     51.212     51.1040    51.1040   \n",
              " \n",
              "    Equity_0_3_Spread  Equity_0_3_Upfront  Mezz_3_7_Spread  Mezz_3_7_Upfront  \\\n",
              " 3             864.68           30.086765           231.14          5.954627   \n",
              " 9             880.98           30.623924           237.64          6.239211   \n",
              " 15            867.85           30.156523           232.13          5.994775   \n",
              " 21            841.94           29.306872              NaN          5.715660   \n",
              " 27            846.72           29.180664           224.75          5.599380   \n",
              " 33            790.79           27.513490              NaN          5.060100   \n",
              " 39             828.7           28.742048              NaN          5.481373   \n",
              " 45            808.13           28.028306           212.24          5.081741   \n",
              " \n",
              "     Mezz_7_10_Spread  Senior_10_15_Spread  SuperSenior_15_100_Spread  \\\n",
              " 3             121.82                61.47                      24.14   \n",
              " 9             124.51                64.20                      22.40   \n",
              " 15            122.72                61.51                      25.21   \n",
              " 21               NaN                  NaN                      23.75   \n",
              " 27            118.23                60.29                      22.03   \n",
              " 33            115.83                58.70                      22.16   \n",
              " 39            114.47                57.81                      21.70   \n",
              " 45            110.50                55.20                      21.76   \n",
              " \n",
              "     Index_0_100_Spread  \n",
              " 3                58.47  \n",
              " 9                57.79  \n",
              " 15               59.51  \n",
              " 21               57.26  \n",
              " 27               55.97  \n",
              " 33               51.73  \n",
              " 39               54.82  \n",
              " 45               53.91  )"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t6NWV4KLYlLU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. MODEL CLASSES (MATH ENGINE)\n",
        "\n",
        "class DiscountCurve:\n",
        "    \"\"\"Handles OIS discounting.\"\"\"\n",
        "    def __init__(self, tenors, rates):\n",
        "        # Rates inputs are expected in decimals (e.g. 0.03 for 3%)\n",
        "        self.times = np.array(tenors)\n",
        "        self.rates = np.array(rates)\n",
        "        # Linear interpolation of Zero Rates\n",
        "        self.rate_interp = interp1d(self.times, self.rates, kind='linear', fill_value='extrapolate')\n",
        "\n",
        "    def get_df(self, t):\n",
        "        if t <= 1e-6: return 1.0\n",
        "        r = self.rate_interp(t)\n",
        "        return np.exp(-r * t)\n",
        "\n",
        "class SurvivalCurve:\n",
        "    \"\"\"Bootstraps survival probabilities from Average CDS Spreads.\"\"\"\n",
        "    def __init__(self, discount_curve, tenors, spreads, recovery=0.4):\n",
        "        self.dc = discount_curve\n",
        "        self.R = recovery\n",
        "        self.times = np.array(tenors)\n",
        "        self.spreads = np.array(spreads) # in decimals\n",
        "        self.survival_probs = self._bootstrap()\n",
        "        self.surv_interp = interp1d(np.insert(self.times, 0, 0), np.insert(self.survival_probs, 0, 1.0),\n",
        "                                    kind='linear', fill_value='extrapolate')\n",
        "\n",
        "    def _bootstrap(self):\n",
        "        # Simplified piecewise constant hazard rate bootstrapping\n",
        "        surv_probs = []\n",
        "        prev_t = 0\n",
        "        prev_surv = 1.0\n",
        "\n",
        "        for i, t in enumerate(self.times):\n",
        "            s = self.spreads[i]\n",
        "            # Risky Annuity approximation: sum(DF * S * dt) = (1-R) * (1 - Surv)\n",
        "            # Detailed bootstrap:\n",
        "            # Protection ~ (1-R) * (prev_surv - S_t) * DF_mid\n",
        "            # Premium ~ S * 0.5 * (prev_surv + S_t) * dt * DF_mid\n",
        "            # Analytic shortcut for Hazard Rate (lambda) ~ S / (1-R)\n",
        "            # We use the shortcut for stability in LHP contexts, or simple bootstrap:\n",
        "\n",
        "            lam = s / (1.0 - self.R)\n",
        "            surv = np.exp(-lam * t)\n",
        "            surv_probs.append(surv)\n",
        "\n",
        "        return np.array(surv_probs)\n",
        "\n",
        "    def get_prob(self, t):\n",
        "        return self.surv_interp(t)\n",
        "\n",
        "class GaussianCopulaLHP:\n",
        "    \"\"\"\n",
        "    Large Homogeneous Portfolio (LHP) Model.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_quad=30):\n",
        "        # Gauss-Hermite Quadrature nodes and weights\n",
        "        self.x_nodes, self.weights = hermgauss(n_quad)\n",
        "        # Normalize weights for standard normal PDF\n",
        "        self.weights /= np.sqrt(np.pi)\n",
        "\n",
        "    def get_conditional_pd(self, p_t, rho, M):\n",
        "        \"\"\"p(t|M) formula\"\"\"\n",
        "        if rho <= 1e-5: return p_t\n",
        "        if rho >= 0.999: return 1.0 if M < norm.ppf(p_t) else 0.0\n",
        "\n",
        "        num = norm.ppf(p_t) - np.sqrt(rho) * np.sqrt(2) * M # sqrt(2) for hermgauss scaling\n",
        "        den = np.sqrt(1 - rho)\n",
        "        return norm.cdf(num / den)\n",
        "\n",
        "    def expected_tranche_loss(self, detachment, rho, p_t):\n",
        "        \"\"\"\n",
        "        Calculate E[min(L, K)] for a base tranche [0, K]\n",
        "        \"\"\"\n",
        "        loss_sum = 0.0\n",
        "        for i, M in enumerate(self.x_nodes):\n",
        "            # Conditional Portfolio Loss %\n",
        "            cond_pd = self.get_conditional_pd(p_t, rho, M)\n",
        "            # LHP assumes Recovery is constant, so Portfolio Loss = cond_pd * (1-R)\n",
        "            # But usually we work with loss fractions.\n",
        "            # Tranche loss cap is 'detachment'\n",
        "\n",
        "            # Note: The standard formula is on Loss Fraction.\n",
        "            # If PD is 10%, Loss is 6% (assuming R=0.4).\n",
        "            # The tranche detachment K is also in Loss terms?\n",
        "            # No, K is usually Notional detachment.\n",
        "            # We must be careful: Is K input as Notional % or Loss %?\n",
        "            # Standard convention: K is Notional detachment (e.g. 0.03).\n",
        "            # Portfolio Loss = PD * (1-R).\n",
        "\n",
        "            port_loss = cond_pd # This is % of defaults\n",
        "            # To compare with K (Notional), we need (1-R) factor?\n",
        "            # Actually, let's keep it clean:\n",
        "            # L_port = cond_pd * (1 - 0.4)\n",
        "            # Tranche Loss = Min(L_port, K)\n",
        "\n",
        "            # However, simpler implementation often treats everything in Notional terms\n",
        "            # and applies (1-R) at the end. Let's stick to the PPT logic:\n",
        "            # We need the Expected Loss Amount.\n",
        "\n",
        "            l_port_val = cond_pd * (1 - 0.4) # Hardcoded R=0.4 per LHP\n",
        "            tranche_loss_val = min(l_port_val, detachment)\n",
        "\n",
        "            loss_sum += self.weights[i] * tranche_loss_val\n",
        "\n",
        "        return loss_sum\n",
        "\n",
        "class CDXPricer:\n",
        "    \"\"\"Prices CDX Tranches.\"\"\"\n",
        "    def __init__(self, discount_curve, survival_curve):\n",
        "        self.df = discount_curve\n",
        "        self.sc = survival_curve\n",
        "        self.model = GaussianCopulaLHP(n_quad=64)\n",
        "        self.R = 0.4\n",
        "\n",
        "    def pv_legs(self, detachment, rho, tenor):\n",
        "        \"\"\"\n",
        "        Calculates PV Protection and PV Risky Annuity for Base Tranche [0, D]\n",
        "        \"\"\"\n",
        "        dt = 0.25\n",
        "        times = np.arange(dt, tenor + dt, dt)\n",
        "\n",
        "        pv_prot = 0.0\n",
        "        pv_ann = 0.0\n",
        "        prev_el = 0.0\n",
        "\n",
        "        for t in times:\n",
        "            df_t = self.df.get_df(t)\n",
        "            surv_t = self.sc.get_prob(t)\n",
        "            p_t = 1.0 - surv_t\n",
        "\n",
        "            # Expected Loss of Base Tranche [0, D]\n",
        "            # Note: We pass rho. If rho is NaN/None, we can't calc.\n",
        "            curr_el = self.model.expected_tranche_loss(detachment, rho, p_t)\n",
        "\n",
        "            # Protection Leg: Integral DF * dEL\n",
        "            dEL = curr_el - prev_el\n",
        "            pv_prot += dEL * df_t\n",
        "\n",
        "            # Premium Leg: Integral DF * (Remaining_Notional) * dt\n",
        "            # Remaining Notional = Detachment - Existing_Loss\n",
        "            curr_rem_notional = detachment - curr_el\n",
        "            pv_ann += curr_rem_notional * dt * df_t\n",
        "\n",
        "            prev_el = curr_el\n",
        "\n",
        "        return pv_prot, pv_ann\n",
        "\n",
        "    def price_tranche(self, attach, detach, rho_a, rho_d, tenor, quote_type='spread'):\n",
        "        \"\"\"\n",
        "        Returns model value for tranche [A, D].\n",
        "        If quote_type='upfront', returns Upfront %.\n",
        "        If quote_type='spread', returns Spread bps.\n",
        "        \"\"\"\n",
        "        # Base [0, D]\n",
        "        prot_d, ann_d = self.pv_legs(detach, rho_d, tenor)\n",
        "\n",
        "        # Base [0, A]\n",
        "        if attach == 0:\n",
        "            prot_a, ann_a = 0.0, 0.0\n",
        "        else:\n",
        "            prot_a, ann_a = self.pv_legs(attach, rho_a, tenor)\n",
        "\n",
        "        leg_prot = prot_d - prot_a\n",
        "        leg_ann = ann_d - ann_a\n",
        "\n",
        "        if leg_ann < 1e-9: return 0.0\n",
        "\n",
        "        if quote_type == 'upfront':\n",
        "            # Upfront = PV_Prot - Running_Coupon * PV_Ann\n",
        "            # Equity runs at 500bps usually\n",
        "            running_coupon = 0.05\n",
        "            val = leg_prot - running_coupon * leg_ann\n",
        "            return val * 100 # Convert to %\n",
        "        else:\n",
        "            # Spread = PV_Prot / PV_Ann\n",
        "            return (leg_prot / leg_ann) * 10000 # Convert to bps"
      ],
      "metadata": {
        "id": "j8mQi7jlYKox"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# ==========================================\n",
        "# 3. MAIN CALIBRATION LOOP\n",
        "# ==========================================\n",
        "\n",
        "def run_calibration():\n",
        "    df_cdx, ois_data, avg_spreads = prepare_data()\n",
        "\n",
        "    # Storage for results\n",
        "    results = []\n",
        "\n",
        "    # Tranche Definitions: (Attach, Detach, QuoteCol, Type)\n",
        "    tranche_defs = [\n",
        "        (0.00, 0.03, 'Equity_0_3_Upfront', 'upfront'),\n",
        "        (0.03, 0.07, 'Mezz_3_7_Spread', 'spread'),\n",
        "        (0.07, 0.10, 'Mezz_7_10_Spread', 'spread'),\n",
        "        (0.10, 0.15, 'Senior_10_15_Spread', 'spread'),\n",
        "        (0.15, 1.00, 'SuperSenior_15_100_Spread', 'spread') # Using 15-100 as approximation for 15-30\n",
        "    ]\n",
        "\n",
        "    print(\"Starting Base Correlation Calibration...\")\n",
        "\n",
        "    # Loop through each date in CDX data\n",
        "    for date in df_cdx['Date']:\n",
        "        if date not in ois_data.index or date not in avg_spreads.index:\n",
        "            continue\n",
        "\n",
        "        # 1. Build Curves\n",
        "        # Discount Curve\n",
        "        day_rates = ois_data.loc[date].dropna()\n",
        "        dc = DiscountCurve(day_rates.index.values, day_rates.values / 100.0) # rates to decimal\n",
        "\n",
        "        # Survival Curve (Representative LHP Curve)\n",
        "        # Using the average spread calculated from constituents\n",
        "        avg_s = avg_spreads.loc[date]\n",
        "        # Create a flat term structure for simplicity or use the 5Y point\n",
        "        sc = SurvivalCurve(dc, [5.0], [avg_s])\n",
        "\n",
        "        # Pricer Instance\n",
        "        pricer = CDXPricer(dc, sc)\n",
        "\n",
        "        # 2. Get Market Data for this date\n",
        "        market_row = df_cdx[df_cdx['Date'] == date].iloc[0]\n",
        "\n",
        "        # 3. Calibrate Base Correlations (Bootstrapping)\n",
        "        # We store Base Correlations: {Detachment: Rho}\n",
        "        base_corrs = {0.0: 0.0} # Attachment 0 has no correlation\n",
        "\n",
        "        row_res = {'Date': date, 'Avg_Spread_bps': avg_s * 10000}\n",
        "\n",
        "        for attach, detach, col, q_type in tranche_defs:\n",
        "            if col not in market_row: continue\n",
        "\n",
        "            quote = market_row[col]\n",
        "            if pd.isna(quote): continue\n",
        "\n",
        "            # Retrieve known rho for attachment point\n",
        "            rho_a = base_corrs.get(attach)\n",
        "\n",
        "            # Objective Function\n",
        "            def objective(rho_d):\n",
        "                if rho_d < 0.01 or rho_d > 0.99: return 1e5\n",
        "                model_val = pricer.price_tranche(attach, detach, rho_a, rho_d, 5.0, q_type)\n",
        "                return model_val - quote\n",
        "\n",
        "            try:\n",
        "                # Solve for rho_d\n",
        "                implied_rho = brentq(objective, 0.01, 0.99)\n",
        "                base_corrs[detach] = implied_rho\n",
        "\n",
        "                # Save Result\n",
        "                row_res[f'Rho_{int(detach*100)}'] = implied_rho\n",
        "                row_res[f'Quote_{int(detach*100)}'] = quote\n",
        "\n",
        "            except Exception as e:\n",
        "                # print(f\"  Failed for {detach} on {date.date()}: {e}\")\n",
        "                row_res[f'Rho_{int(detach*100)}'] = np.nan\n",
        "\n",
        "        results.append(row_res)\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# ==========================================\n",
        "# 4. VISUALIZATION\n",
        "# ==========================================\n",
        "\n",
        "def plot_results(df_res):\n",
        "    if df_res.empty:\n",
        "        print(\"No results to plot.\")\n",
        "        return\n",
        "\n",
        "    # Set Date Index\n",
        "    df_res.set_index('Date', inplace=True)\n",
        "\n",
        "    # 1. Base Correlation Time Series\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    cols = [c for c in df_res.columns if 'Rho_' in c]\n",
        "    for c in cols:\n",
        "        plt.plot(df_res.index, df_res[c], label=f\"Base Corr {c.replace('Rho_', '')}%\")\n",
        "\n",
        "    plt.title('Base Correlation Term Structure (Time Series)')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # 2. Correlation Smile (Last Available Date)\n",
        "    last_date = df_res.index[-1]\n",
        "    last_row = df_res.loc[last_date]\n",
        "\n",
        "    detachments = []\n",
        "    rhos = []\n",
        "\n",
        "    for c in cols:\n",
        "        detachment = float(c.replace('Rho_', ''))\n",
        "        rho = last_row[c]\n",
        "        if not np.nan:\n",
        "            detachments.append(detachment)\n",
        "            rhos.append(rho)\n",
        "\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(detachments, rhos, 'o-', linewidth=2, markersize=8, color='darkred')\n",
        "    plt.title(f'Base Correlation Smile (as of {last_date.date()})')\n",
        "    plt.xlabel('Detachment Point (%)')\n",
        "    plt.ylabel('Base Correlation')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    # Annotate points\n",
        "    for x, y in zip(detachments, rhos):\n",
        "        plt.text(x, y+0.01, f\"{y:.2f}\", ha='center')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# EXECUTION\n",
        "# ==========================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df_results = run_calibration()\n",
        "    print(\"Calibration Complete.\")\n",
        "    print(df_results.head())\n",
        "\n",
        "    plot_results(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "8-8Hsbkw5S1H",
        "outputId": "fb8298cb-7c24-43a9-d1a2-e7b7d3ab2989"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'prepare_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2964410827.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m     \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calibration Complete.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2964410827.py\u001b[0m in \u001b[0;36mrun_calibration\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_calibration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mdf_cdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mois_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_spreads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Storage for results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'prepare_data' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gaussian Copula + Base correlatiion Hedge methods:**\n",
        "\n",
        "1. Spread risks: **Delta hedge**\n",
        "\n",
        "2. Correlation risks: hedgd by correlation swaps\n",
        "\n",
        "3. Gamma (tail) risks: Deep OTM index puts (Macro overlay)\n",
        "\n",
        "**Stress Test:**\n",
        "\n",
        "Measure loss by changing correlation $\\rho$ & spreads\n",
        "\n",
        "**Shocks solutions:**\n",
        "\n",
        "1. Stress Delta (computed from stress test)\n",
        "\n",
        "2. Compare to shadow models (e.g. GVG, Stochastic recovery)\n",
        "\n",
        "3. Macro hedges/ Convexity :Deep OTM index puts, Correlation Swaps\n",
        "\n",
        "4. De-risking: Unwind"
      ],
      "metadata": {
        "id": "YCtEshJ9PZ3t"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oucGR7ErPfBt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}